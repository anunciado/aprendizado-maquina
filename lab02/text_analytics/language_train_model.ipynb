{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The training data folder must be passed as first argument\n",
    "languages_data_folder = './data/languages/paragraphs'\n",
    "dataset = load_files(languages_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset in training and test set:\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a vectorizer that splits strings into sequence of 1 to 3 characters instead of word tokens\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char', use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a vectorizer / classifier pipeline using the previous analyzer\n",
    "# the pipeline instance should stored in a variable named clf\n",
    "clf = Pipeline([('vec', vectorizer),('clf', Perceptron()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vec', TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ...n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
       "      verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline on the training set\n",
    "clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the outcome on the testing set in a variable named y_predicted\n",
    "y_predicted = clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         ar       1.00      1.00      1.00        12\n",
      "         de       1.00      1.00      1.00        87\n",
      "         en       1.00      1.00      1.00        81\n",
      "         es       1.00      0.95      0.97        57\n",
      "         fr       0.97      1.00      0.98        61\n",
      "         it       0.97      1.00      0.99        37\n",
      "         ja       1.00      1.00      1.00        38\n",
      "         nl       1.00      1.00      1.00        23\n",
      "         pl       1.00      1.00      1.00        20\n",
      "         pt       1.00      1.00      1.00        50\n",
      "         ru       1.00      1.00      1.00        25\n",
      "\n",
      "avg / total       0.99      0.99      0.99       491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 87  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 81  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 54  2  1  0  0  0  0  0]\n",
      " [ 0  0  0  0 61  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 38  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 23  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 50  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 25]]\n"
     ]
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the result on some short new sentences:\n",
    "sentences = [\n",
    "    u'I am Luís Eduardo and i am a student.',\n",
    "    u'Soy Luis Eduardo una chica.',\n",
    "    u'Je suis Luís Eduardo ça va.',\n",
    "    u'Я Луис Эдуардо.',\n",
    "    u'私はです.',\n",
    "    u'Meu nome é Luís Eduardo.',\n",
    "    u'Ich heiße Luís Eduardo und ich komme aus Brasilien.',\n",
    "]\n",
    "predicted = clf.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language of \"I am Luís Eduardo and i am a student.\" is \"pt\"\n",
      "The language of \"Soy Luis Eduardo una chica.\" is \"es\"\n",
      "The language of \"Je suis Luís Eduardo ça va.\" is \"pt\"\n",
      "The language of \"Я Луис Эдуардо.\" is \"ru\"\n",
      "The language of \"私はです.\" is \"ja\"\n",
      "The language of \"Meu nome é Luís Eduardo.\" is \"pt\"\n",
      "The language of \"Ich heiße Luís Eduardo und ich komme aus Brasilien.\" is \"de\"\n"
     ]
    }
   ],
   "source": [
    "for s, p in zip(sentences, predicted):\n",
    "    print(u'The language of \"%s\" is \"%s\"' % (s, dataset.target_names[p]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
